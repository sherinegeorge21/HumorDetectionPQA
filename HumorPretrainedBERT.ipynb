{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0020c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grbha\\anaconda3\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\grbha\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grbha\\anaconda3\\Lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\grbha\\anaconda3\\Lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re  # for regex\n",
    "import seaborn as sns\n",
    "import os\n",
    "import unidecode\n",
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, confusion_matrix, \n",
    "                             ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, make_scorer)\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# import tensorflow_hub as hub\n",
    "# import bert_tokenization as tokenization\n",
    "from transformers import *\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from nltk.corpus import stopwords\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1135b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading configuration file bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ColBERT\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {\n",
      "    \"col_pooling\": {\n",
      "      \"activation_function\": \"tanh\",\n",
      "      \"head_type\": \"tagging\",\n",
      "      \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1,\n",
      "        \"LABEL_10\": 10,\n",
      "        \"LABEL_11\": 11,\n",
      "        \"LABEL_12\": 12,\n",
      "        \"LABEL_13\": 13,\n",
      "        \"LABEL_14\": 14,\n",
      "        \"LABEL_15\": 15,\n",
      "        \"LABEL_16\": 16,\n",
      "        \"LABEL_17\": 17,\n",
      "        \"LABEL_18\": 18,\n",
      "        \"LABEL_19\": 19,\n",
      "        \"LABEL_2\": 2,\n",
      "        \"LABEL_20\": 20,\n",
      "        \"LABEL_21\": 21,\n",
      "        \"LABEL_22\": 22,\n",
      "        \"LABEL_23\": 23,\n",
      "        \"LABEL_24\": 24,\n",
      "        \"LABEL_25\": 25,\n",
      "        \"LABEL_26\": 26,\n",
      "        \"LABEL_27\": 27,\n",
      "        \"LABEL_28\": 28,\n",
      "        \"LABEL_29\": 29,\n",
      "        \"LABEL_3\": 3,\n",
      "        \"LABEL_30\": 30,\n",
      "        \"LABEL_31\": 31,\n",
      "        \"LABEL_4\": 4,\n",
      "        \"LABEL_5\": 5,\n",
      "        \"LABEL_6\": 6,\n",
      "        \"LABEL_7\": 7,\n",
      "        \"LABEL_8\": 8,\n",
      "        \"LABEL_9\": 9\n",
      "      },\n",
      "      \"layers\": 1,\n",
      "      \"num_labels\": 32\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert were not used when initializing BertModel: ['heads.col_pooling.1.bias', 'heads.col_pooling.1.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertModel, BertConfig\n",
    "\n",
    "\n",
    "tf_checkpoint_path = 'bert'\n",
    "MAX_SIZE = 200\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(tf_checkpoint_path)\n",
    "model = BertModel.from_pretrained(tf_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce1549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a282f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "humorousQ = pd.read_csv(\"Humorous.csv\")\n",
    "non_humorousQ = pd.read_csv(\"Non-humorous-unbiased.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b3e22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Will the volca sample get me a girlfriend?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can u communicate with spirits even on Saturday?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I won't get hunted right?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have a few questions.. Can you get possessed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Has anyone asked where the treasure is? What w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19137</th>\n",
       "      <td>Serve na f800r 2013?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19138</th>\n",
       "      <td>Can it run mine sweeper abve 10 fps?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19139</th>\n",
       "      <td>What is the difference between the pro weight ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>Can you provide me a phone number to the compa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>Is the blenders sponge latex free?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19142 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  humor\n",
       "0             Will the volca sample get me a girlfriend?      1\n",
       "1       Can u communicate with spirits even on Saturday?      1\n",
       "2                              I won't get hunted right?      1\n",
       "3      I have a few questions.. Can you get possessed...      1\n",
       "4      Has anyone asked where the treasure is? What w...      1\n",
       "...                                                  ...    ...\n",
       "19137                               Serve na f800r 2013?      0\n",
       "19138               Can it run mine sweeper abve 10 fps?      0\n",
       "19139  What is the difference between the pro weight ...      0\n",
       "19140  Can you provide me a phone number to the compa...      0\n",
       "19141                 Is the blenders sponge latex free?      0\n",
       "\n",
       "[19142 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.concat([humorousQ, non_humorousQ], ignore_index=True)\n",
    "questions.rename(columns={'label': 'humor'}, inplace=True)\n",
    "questions.drop(['product_description', 'image_url'], axis=1, inplace=True)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa648941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grbha\\AppData\\Local\\Temp\\ipykernel_24256\\4241353287.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  questions.text =  [re.sub(r\"[^a-zA-Z]\",\" \",text) for text in questions.question.values]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                question  humor\n",
      "0             will the volca sample get me a girlfriend?      1\n",
      "1      can you communicate with spirits even on satur...      1\n",
      "2                           i will not get hunted right?      1\n",
      "3      i have a few questions.. can you get possessed...      1\n",
      "4      has anyone asked where the treasure is? what w...      1\n",
      "...                                                  ...    ...\n",
      "19137                               serve na f800r 2013?      0\n",
      "19138              can it run mine sweeper above 10 fps?      0\n",
      "19139  what is the difference between the pro weight ...      0\n",
      "19140  can you provide me a phone number to the compa...      0\n",
      "19141                 is the blenders sponge latex free?      0\n",
      "\n",
      "[19142 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def case_convert():\n",
    "    questions.question = [i.lower() for i in questions.question.values]\n",
    "\n",
    "def remove_specials():\n",
    "    questions.text =  [re.sub(r\"[^a-zA-Z]\",\" \",text) for text in questions.question.values]\n",
    "\n",
    "def remove_shorthands():\n",
    "    CONTRACTION_MAP = {\n",
    "    \"u\": \"you\",\n",
    "    \"abve\": \"above\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    texts = []\n",
    "    for text in questions.question.values:\n",
    "        string = \"\"\n",
    "        for word in text.split(\" \"):\n",
    "            if word.strip() in list(CONTRACTION_MAP.keys()):\n",
    "                string = string + \" \" + CONTRACTION_MAP[word]\n",
    "            else:\n",
    "                string = string + \" \" + word\n",
    "        texts.append(string.strip())\n",
    "    questions.question = texts\n",
    "\n",
    "def remove_stopwords():\n",
    "    texts = []\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    for item in questions.question.values:\n",
    "        string = \"\"\n",
    "        for word in item.split(\" \"):\n",
    "            if word.strip() in stopwords_list:\n",
    "                continue\n",
    "            else:\n",
    "                string = string + \" \" + word\n",
    "        texts.append(string)\n",
    "                \n",
    "def remove_links():\n",
    "    texts = []\n",
    "    for text in questions.question.values:\n",
    "        remove_https = re.sub(r'http\\S+', '', text)\n",
    "        remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "        texts.append(remove_com)\n",
    "    questions.question = texts\n",
    "\n",
    "def remove_accents():\n",
    "    questions.question = [unidecode.unidecode(text) for text in questions.question.values]\n",
    "\n",
    "def normalize_spaces():\n",
    "    questions.question = [re.sub(r\"\\s+\",\" \",text) for text in questions.question.values]\n",
    "\n",
    "case_convert()\n",
    "remove_links()\n",
    "remove_shorthands()\n",
    "remove_accents()\n",
    "remove_specials()\n",
    "remove_stopwords()\n",
    "normalize_spaces()\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7f934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(questions, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b780146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19142 15313 3829\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>can i use these to fight crime or will the oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>is there any chance that this tie can make ame...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17908</th>\n",
       "      <td>does it come packaged in its original package?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>does this product prevent broken hearts?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>if i seed over established st. augustine will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  humor\n",
       "8159   can i use these to fight crime or will the oth...      1\n",
       "3574   is there any chance that this tie can make ame...      1\n",
       "17908     does it come packaged in its original package?      0\n",
       "5948            does this product prevent broken hearts?      1\n",
       "6602   if i seed over established st. augustine will ...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16437</th>\n",
       "      <td>what kind of horn is this made from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11612</th>\n",
       "      <td>is this one single ticket? i need a roll of them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>do my kids need the hepatitis vaccine before p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>why would you buy this its probably a small qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>can it make me a sandwich?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question\n",
       "16437               what kind of horn is this made from?\n",
       "11612  is this one single ticket? i need a roll of them.\n",
       "6657   do my kids need the hepatitis vaccine before p...\n",
       "11036  why would you buy this its probably a small qu...\n",
       "9058                          can it make me a sandwich?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df_y = df_test.copy()\n",
    "del df_test['humor']\n",
    "\n",
    "df_sub = test_df_y.copy()\n",
    "\n",
    "print(len(questions),len(df_train),len(df_test))\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81761a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input categories:\n",
      "\t ['question']\n",
      "\n",
      "output TARGET_COUNT:\n",
      "\t 1\n",
      "\n",
      "output categories:\n",
      "\t ['humor']\n"
     ]
    }
   ],
   "source": [
    "output_categories = list(df_train.columns[[1]])\n",
    "input_categories = list(df_train.columns[[0]])\n",
    "\n",
    "TARGET_COUNT = len(output_categories)\n",
    "\n",
    "print('\\ninput categories:\\n\\t', input_categories)\n",
    "print('\\noutput TARGET_COUNT:\\n\\t', TARGET_COUNT)\n",
    "print('\\noutput categories:\\n\\t', output_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7143188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grbha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee8c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "    inputs = tokenizer.encode_plus(str1, str2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=length,\n",
    "        truncation_strategy=truncation_strategy)\n",
    "\n",
    "    input_ids =  inputs[\"input_ids\"]\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    padding_length = length - len(input_ids)\n",
    "    padding_id = tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer):\n",
    "    model_input = []\n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input.append([])\n",
    "    \n",
    "    for _, row in tqdm(df[columns].iterrows()):\n",
    "        i = 0\n",
    "        \n",
    "        # sent\n",
    "        sentences = sent_tokenize(row.question)\n",
    "        for xx in range(MAX_SENTENCES):\n",
    "            s = sentences[xx] if xx<len(sentences) else ''\n",
    "            ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "            model_input[i].append(ids_q)\n",
    "            i+=1\n",
    "            model_input[i].append(masks_q)\n",
    "            i+=1\n",
    "            model_input[i].append(segments_q)\n",
    "            i+=1\n",
    "        \n",
    "        # full row\n",
    "        ids_q, masks_q, segments_q = return_id(row.question, None, 'longest_first', MAX_LENGTH)\n",
    "        model_input[i].append(ids_q)\n",
    "        i+=1\n",
    "        model_input[i].append(masks_q)\n",
    "        i+=1\n",
    "        model_input[i].append(segments_q)\n",
    "        \n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "        \n",
    "    print(model_input[0].shape)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942f67e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebab6e385694feeb0d62eba2111b9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15313, 20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7066a5350e10467ea95271728b1cc88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3829, 20)\n"
     ]
    }
   ],
   "source": [
    "inputs      = compute_input_arrays(df_train, input_categories, tokenizer)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157f7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 15313 20\n",
      "the second picture shows serrations. so this knife comes with them?\n",
      "['the second picture shows serrations.', 'so this knife comes with them?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  101,  1996,  2117,  3861,  3065, 22737,  9285,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]),\n",
       " array([ 101, 2061, 2023, 5442, 3310, 2007, 2068, 1029,  102,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " array([101, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([  101,  1996,  2117,  3861,  3065, 22737,  9285,  1012,  2061,\n",
       "         2023,  5442,  3310,  2007,  2068,  1029,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(inputs), len(inputs[0]), len(inputs[0][0]))\n",
    "\n",
    "# check out input for 7th row\n",
    "xx = 7\n",
    "print(df_train.iloc[xx,0])\n",
    "print(sent_tokenize(df_train.iloc[xx,0]))\n",
    "inputs[0][xx], inputs[3][xx], inputs[6][xx], inputs[15][xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a476a266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be56ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "import sklearn\n",
    "def print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n",
    "    print('==================', label2)\n",
    "    ### For regression\n",
    "    if is_regression:\n",
    "        print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n",
    "        print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "        print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n",
    "        #     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n",
    "        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "    else:\n",
    "        ### FOR Classification\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "        print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n",
    "        \n",
    "        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "        print(matrix)\n",
    "        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n",
    "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n",
    "        return sklearn.metrics.accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c0166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs = inputs\n",
    "valid_outputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a26df90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(valid_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "preds = model.predict(valid_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afcb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
